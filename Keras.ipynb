{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import keras.utils.np_utils as ku\n",
    "import keras.models as models\n",
    "import keras.layers as layers\n",
    "from keras import regularizers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_samp, train_lab), (test_samp, test_lab) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_samp = train_samp.reshape(60000, 28 * 28 ).astype('float32')/255 # divding by 255 cause each pixel vary\n",
    "test_samp = test_samp.reshape(10000, 28 * 28).astype('float')/255      # between 0 and 255 so dividing changes\n",
    "                                                             # it to vary bertween 0 to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding layers\n",
    "# 1. convulation layer adds a filter to each term of the tensor\n",
    "# 2. relu converts all the -ve numbers to 0\n",
    "# 3. maxpooling takes multiple values from a tensor and creates a new tensor \n",
    "# with the average of those values\n",
    "# 4. fully connected layer takes all of the tensors after the maxpooling and reshapes them into\n",
    "# a vector\n",
    "# 5.softmax layer takes all of the data from the vectors created by the fully connected layers and\n",
    "# creates a probability distribution table and determines the output i.e the value of x with the \n",
    "# highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.2730 - accuracy: 0.9206 - val_loss: 0.1404 - val_accuracy: 0.9576\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.1162 - accuracy: 0.9652 - val_loss: 0.0989 - val_accuracy: 0.9697\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0804 - accuracy: 0.9762 - val_loss: 0.0811 - val_accuracy: 0.9748\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 6s 101us/step - loss: 0.0643 - accuracy: 0.9806 - val_loss: 0.0724 - val_accuracy: 0.9781\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.0523 - accuracy: 0.9841 - val_loss: 0.0711 - val_accuracy: 0.9783\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.0434 - accuracy: 0.9873 - val_loss: 0.0631 - val_accuracy: 0.9815\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0362 - accuracy: 0.9888 - val_loss: 0.0672 - val_accuracy: 0.9804\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 7s 125us/step - loss: 0.0314 - accuracy: 0.9902 - val_loss: 0.0681 - val_accuracy: 0.9811\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0271 - accuracy: 0.9916 - val_loss: 0.0691 - val_accuracy: 0.9818\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 7s 108us/step - loss: 0.0242 - accuracy: 0.9926 - val_loss: 0.0651 - val_accuracy: 0.9824\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.0725 - val_accuracy: 0.9821\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0186 - accuracy: 0.9941 - val_loss: 0.0731 - val_accuracy: 0.9828\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0156 - accuracy: 0.9949 - val_loss: 0.0778 - val_accuracy: 0.9798\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 8s 132us/step - loss: 0.0157 - accuracy: 0.9950 - val_loss: 0.0763 - val_accuracy: 0.9825\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.0138 - accuracy: 0.9957 - val_loss: 0.0735 - val_accuracy: 0.9813\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0115 - accuracy: 0.9962 - val_loss: 0.0753 - val_accuracy: 0.9823\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 8s 135us/step - loss: 0.0100 - accuracy: 0.9966 - val_loss: 0.0752 - val_accuracy: 0.9828\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 7s 124us/step - loss: 0.0098 - accuracy: 0.9970 - val_loss: 0.0773 - val_accuracy: 0.9845\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 7s 114us/step - loss: 0.0092 - accuracy: 0.9970 - val_loss: 0.0811 - val_accuracy: 0.9829\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 7s 116us/step - loss: 0.0080 - accuracy: 0.9974 - val_loss: 0.0851 - val_accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "nn = models.Sequential()\n",
    "nn.add(layers.Dense(512, activation = 'relu', input_shape = (28 * 28,))) \n",
    "nn.add(layers.Dropout(0.2))\n",
    "nn.add(layers.Dense(10, activation = 'softmax', input_shape = (28 * 28,)))\n",
    "nn.compile(optimizer = 'rmsprop', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "history = nn.fit(train_samp, train_lab, epochs = 20, batch_size = 128, validation_data = (test_samp, test_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 784)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = cv2.imread('Untitled.png', 0)\n",
    "t = t/255\n",
    "l = 5\n",
    "t = t.reshape(1, 784)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict_classes(t)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
